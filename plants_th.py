from __future__ import print_function
import keras

from vgg_modif import VGG16

from keras.datasets import cifar10
from keras.utils import Sequence
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D
import os
import numpy as np
from matplotlib import pyplot as plt
from tqdm import tqdm
import keras.backend as K
import seaborn as sns
import pandas as pd
import sklearn
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
from IPython.display import clear_output
from keras.callbacks import CSVLogger, ModelCheckpoint
import json
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD
import theano as th
from pprint import pprint
import PIL

from PIL import Image
from skimage.io import imread
from skimage.transform import resize
import numpy as np
from skimage.io import imread

# Here, `x_set` is list of path to the images
# and `y_set` are the associated classes.
batch_size = 8
crop_length = 224


def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    assert img.shape[0] == 3
    height, width = img.shape[1], img.shape[2]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y + dy), x:(x + dx), :]


def VGG_16(weights_path=None):
    model = Sequential()
    model.add(ZeroPadding2D((1, 1), input_shape=(3, 224, 224)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering="th"))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering="th"))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering="th"))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering="th"))

    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1, 1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering="th"))

    model.add(Flatten())
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(4096, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax', name='last'))

    if weights_path:
        model.load_weights(weights_path, by_name=True)

    return model


'''
def crop_generator(batches, crop_length):

    Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator

    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)
'''


def do_stuff():
    with open('/mnt/RAID5/datasets/private/plant-disease/dataset_all/merged/ALLCROPS/image_list.json') as json_data:
        d = json.load(json_data)

    imagenes_trigo = []
    labels = []
    for i in tqdm(d):
        if d[i]["crop"] == "TRZAW":
            image_trig = '/mnt/RAID5/datasets/private/plant-disease/dataset_all/merged/ALLCROPS' + \
                         d[i]["diseases"]["SEPTTR"]["image_path"]
            # open_im = Image.open(image_trig)
            # open_im = open_im.resize((256,256), Image.ANTIALIAS)
            if i[len(i) - 4:len(i)] != ".JPG" and i[len(i) - 4:len(i)] != ".jpg":
                b = i + ".jpg"
            else:
                b = i
            xxx = os.path.join('trigo_resize', b)
            # open_im.save(xxx)
            imagenes_trigo.append(xxx)
            if d[i]["diseases"]["SEPTTR"]["disease_presence"] == True:
                labels.append([0, 1])
            else:
                labels.append([1, 0])

    x_train, x_test, y_train, y_test = train_test_split(imagenes_trigo, labels, test_size=0.25)

    class PlantasTrigoDataset(Sequence):
        def __init__(self, imagenes_trigo, labels):
            self.x_normal = np.array([imread(i) for i in tqdm(imagenes_trigo)])
            self.suma = np.ones(self.x_normal.shape) * 128
            self.x = self.x_normal - self.suma
            self.y = np.array(labels)
            self.batch_size = 32

        def __len__(self):
            return int(np.ceil(self.x.shape[0] / float(self.batch_size)))

        def __getitem__(self, idx):
            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
            batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]
            batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
            for i in range(batch_x.shape[0]):
                batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
            return batch_crops, batch_y
            # np.array([file_name for file_name in batch_x]), np.array(batch_y)

        def get_steps(self):
            return self.x.shape[0] // self.batch_size

    data_train = PlantasTrigoDataset(x_train, y_train)
    data_val = PlantasTrigoDataset(x_test, y_test)
    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)
    # show class indices
    model = VGG_16('vgg16_weights.h5')

    '''
    inputs = (256, 256, 3)
    model_sincabeza = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=inputs, pooling=None, classes=2)
    # Classification block
    my_input = Input(batch_shape=model_sincabeza.output_shape)
    x = Flatten(name='flatten')(my_input)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dropout(0.25)(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dropout(0.25)(x)
    my_output = Dense(2, activation='softmax', name='predictions')(x)
    # opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)
    modelo_cabeza = Model(inputs=my_input, outputs=my_output,name='top_model')
    block5_pool = model_sincabeza.get_layer('block5_pool').output
    full_output = modelo_cabeza(block5_pool)
    full_model = Model(inputs=model_sincabeza.input, outputs=full_output)
    for layer in full_model.layers[:18]:
        layer.trainable = False

    full_model.summary()

    full_model.compile(loss='categorical_crossentropy',
                          optimizer=opt,
                          metrics=['accuracy'])
    full_model.summary()

    csv_logger = CSVLogger('PlantsPretrainDrop.csv', append=True, separator=';')
    filepath = 'WeightsPretrainDrop.hdf5'
    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    full_model.fit_generator(
                    data_train,
                    steps_per_epoch=data_train.get_steps(),
                    epochs=20,
                    validation_data=data_val,validation_steps=data_val.get_steps(),callbacks=[csv_logger,checkpoint])


    inputs = (256, 256, 3)
    model_sincabeza = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=inputs, pooling=None, classes=2)
    # Classification block
    my_input = Input(batch_shape=model_sincabeza.output_shape)
    x = Flatten(name='flatten')(my_input)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dropout(0.25)(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dropout(0.25)(x)
    my_output = Dense(2, activation='softmax', name='predictions')(x)
    # opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)
    modelo_cabeza = Model(inputs=my_input, outputs=my_output,name='top_model')
    block5_pool = model_sincabeza.get_layer('block5_pool').output
    full_output = modelo_cabeza(block5_pool)
    full_model2 = Model(inputs=model_sincabeza.input, outputs=full_output)
    full_model2.summary()

    full_model2.compile(loss='categorical_crossentropy',
                          optimizer=opt,
                          metrics=['accuracy'])
    full_model2.summary()
    full_model2.load_weights('WeightsPretrain.hdf5')
    csv_logger = CSVLogger('plants_trained_with2drop.csv', append=True, separator=';')
    filepath = "weights_plants.hdf5"
    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    full_model2.fit_generator(
        data_train,
        steps_per_epoch=data_train.get_steps(),
        epochs=500,
        validation_data=data_val,
        validation_steps=data_val.get_steps(), callbacks=[csv_logger, checkpoint])

    input2 = Input(shape=(254, 254, 3))
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(input2)
    x = Dropout(0.25)(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)
    x = Dropout(0.25)(x)

    # Block 2
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)
    x = Dropout(0.25)(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)
    x = Dropout(0.25)(x)

    # Block 3
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)
    x = Dropout(0.25)(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)
    x = Dropout(0.25)(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)
    x = Dropout(0.25)(x)

    # Block 4
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)
    x = Dropout(0.25)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)
    x = Dropout(0.25)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)
    x = Dropout(0.25)(x)

    # Block 5
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)
    x = Dropout(0.25)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)
    x = Dropout(0.25)(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)
    x = Dropout(0.25)(x)

    # Top
    x = Flatten(name='flatten')(x)
    x = Dense(4096, activation='relu', name='fc1')(x)
    x = Dropout(0.25)(x)
    x = Dense(4096, activation='relu', name='fc2')(x)
    x = Dropout(0.25)(x)
    output_drop = Dense(2, activation='softmax', name='predictions')(x)
    model = Model(inputs=input2,outputs=output_drop)
    '''
    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy',
                  optimizer=sgd,
                  metrics=['accuracy'])
    model.summary()
    csv_logger = CSVLogger('plants_good.csv', append=True, separator=';')
    filepath = "weights_good.hdf5"
    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    model.fit_generator(
        data_train,
        steps_per_epoch=data_train.get_steps(),
        epochs=500,
        validation_data=data_val,
        validation_steps=data_val.get_steps(),
        callbacks=[csv_logger, checkpoint])

os.environ["THEANO_FLAGS"] = "mode=FAST_RUN,device=gpu,floatX=float32"
do_stuff()